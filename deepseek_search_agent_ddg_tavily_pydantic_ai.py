# -*- coding: utf-8 -*-
"""DeepSeek Search Agent DDG Tavily - Pydantic AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a637WCHcZjd23Bl3ouMUaCp34x1F_NSt

# Building Structured Agents with Pydantic AI and DeepSeek
"""

!pip -q install pydantic-ai
!pip -q install nest_asyncio
!pip -q install devtools
!pip -q install tavily-python
#uses pendatic AI , could use lnagrgrph or langchain

"""### **Restart the notebook**

You will need to restart the notebook after you've installed Pydantic AI and the other dependencies above.
"""

import os
from google.colab import userdata
from IPython.display import display, Markdown

os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
os.environ["GEMINI_API_KEY"] = userdata.get('GOOGLE_AI_STUDIO')
os.environ["GROQ_API_KEY"] = userdata.get('GROQ_API_KEY')
os.environ["DEEPSEEK_API_KEY"] = userdata.get('DEEPSEEK_API_KEY')
os.environ["TAVILY_API_KEY"] = userdata.get('TAVILY_API_KEY') # If you dont have this use the DDGS alternative below

import nest_asyncio
nest_asyncio.apply()

"""## Setting Up search Tavily

### Tavily Search
"""

from tavily import TavilyClient, AsyncTavilyClient

# Setup the Tavily Client
tavily_client = AsyncTavilyClient(api_key=os.environ["TAVILY_API_KEY"])

# Simple Search
response = await tavily_client.search("What is DeepSeekR1?", max_results=3)

print(response['results'])

# Responses
for result in response['results']:
    print(result['title'])
    print(result['content'])
    print('\n')

# RAG Context search

# Context Search
context = await tavily_client.get_search_context(query="What is DeepSeekR1?", max_results=3)

print(context)

"""## Setting up the Agent & Tools"""

from __future__ import annotations as _annotations

import asyncio
import os
from dataclasses import dataclass
from typing import Any

from devtools import debug
from httpx import AsyncClient
import datetime

from pydantic_ai import Agent, ModelRetry, RunContext
from pydantic import BaseModel, Field

"""### Setting up DeepSeek"""

from pydantic_ai import Agent#pandemicsetup stiff : settting up the two models
from pydantic_ai.models.openai import OpenAIModel #deepseek has thier stuff setup same as openAI

# DeepSeekv3
deepseek_chat_model = OpenAIModel( #define the base as open AI
    'deepseek-chat',
    base_url='https://api.deepseek.com',
    api_key=os.environ["DEEPSEEK_API_KEY"],
)

# DeepSeekR1
deepseek_reasoner_model = OpenAIModel(  #reasoning model
    'deepseek-reasoner',
    base_url='https://api.deepseek.com',
    api_key=os.environ["DEEPSEEK_API_KEY"],
)

@dataclass
class SearchDataclass:
    max_results: int
    todays_date: str

@dataclass
class ResearchDependencies:
    todays_date: str

class ResearchResult(BaseModel):
    research_title: str = Field(description='This is a top level Markdown heading that covers the topic of the query and answer prefix it with #')
    research_main: str = Field(description='This is a main section that provides answers for the query and research')
    research_bullets: str = Field(description='This is a set of bulletpoints that summarize the answers for query')

## Make the agent : what we passs in
search_agent = Agent(deepseek_chat_model,
                     deps_type=ResearchDependencies,
                     result_type=ResearchResult,
                     system_prompt='Your a helpful research assistant, you are an expert in research '
                     'If you are given a question you write strong keywords to do 3-5 searches in total '
                     '(each with a query_number) and then combine the results' )

@search_agent.tool #Tavily : Seting up the search tool etc
async def get_search(search_data:RunContext[SearchDataclass],query: str, query_number: int) -> dict[str, Any]:
    """Get the search for a keyword query.

    Args:
        query: keywords to search.
    """
    print(f"Search query {query_number}: {query}")
    max_results = search_data.deps.max_results
    results = await tavily_client.get_search_context(query=query, max_results=max_results)

    return results

## set up the dependencies

# Get the current date
current_date = datetime.date.today()

# Convert the date to a string
date_string = current_date.strftime("%Y-%m-%d")


deps = SearchDataclass(max_results=3, todays_date=date_string)

result = await search_agent.run(
    'can you give me a very detailed breakdown of the DeepSeekR1 model', deps=deps
) #using v3 model at this point

print(result.data)

result.data.research_title = '#'+result.data.research_title
print(result.data.research_title)

print(result.data.research_main)

print(result.data.research_bullets)

combined_markdown = "\n\n".join([result.data.research_title, result.data.research_main, result.data.research_bullets])

Markdown(combined_markdown)

debug(result)

"""## Using DeepSeekR1"""

class LifeMeaningStructuredResult(BaseModel):
    life_meaning_title: str = Field(description='This is a top level Markdown heading that covers the topic of the query and answer prefix it with #')
    life_meaning_main: str = Field(description='This is a main section that provides answers for the query and questions')
    life_meaning_bullets: str = Field(description='This is a set of bulletpoints that summarize the answers for query')

## Make the agent
reasoner_agent = Agent(deepseek_reasoner_model, #swapping out the model for R1 reasnoning mode. we got a tool etc
                     result_type=LifeMeaningStructuredResult,
                     system_prompt='Your a helpful and wise reasoning assistant, you are an expert in thinking '
                     'If you are given a question you think carefully and then respond with a title, your thinking, '
                     'a set of bullets points summary and a final answer '
                      )

result = await reasoner_agent.run('What is the meaning of life?') #it doesnt work cz deepseek reasoner R1 doesnt support function calling at this point

#running it again without any dependecies , just getting response back from the models
## Make the agent
reasoner_agent = Agent(deepseek_reasoner_model, #

                    #  deps_type=ResearchDependencies,
                    #  result_type=LifeMeaningStructuredResult,
                     system_prompt='Your a helpful and wise reasoning assistant, you are an expert in thinking '
                     'If you are given a question you think carefully and then respond with a title, your thinking, '
                     'a set of bullets points summary and a final answer '
                       )

result = await reasoner_agent.run('What is the meaning of life?') #that works..if we dont put any input dependencie

result.all_messages_json()

Markdown(result.data)

Markdown(result.data) #does the work, but we cant index into this.. but It understood the prompt adn formatted it.. we cpould user parsers but wont work all the time

"""## Setting up a helper LLM"""

#Little hack solution is to setup another LLM thats really cheap, that first gives us structured output back, and we can parse in the result type and give this modela  system prompt. We use purefly to format the output we have up there

from pydantic_ai import Agent

class LifeMeaningStructuredResult(BaseModel):
    title: str = Field(description='This is a top level Markdown heading that covers the topic of the query and answer prefix it with #')
    answer: str = Field(description='This is a main section that provides answers for the query and questions')
    bullets: str = Field(description='This is a set of bulletpoints that summarize the answers for query')
    thinking: str = Field(description='This is a string that covers the thinking behind the answer')

formatting_agent = Agent('google-gla:gemini-1.5-flash',
              result_type=LifeMeaningStructuredResult,
              system_prompt='Your a helpful formatting assistant, you never give your own opinion. '
              'You merely take the input given and convert it to the structured result for sending back, '
              )

structured_results = await formatting_agent.run(result.data)

type(structured_results.data.title)

Markdown(structured_results.data.title)

Markdown(structured_results.data.answer)

Markdown(structured_results.data.bullets)

Markdown(structured_results.data.thinking)

"""## Getting the real thinking CoT"""

#not asking fir its thinking anynore
SYSTEM_PROMPT="""Your a helpful and wise reasoning assistant, you are an expert in thinking \
If you are given a question you think carefully and then respond with a title, \
a set of bullets points summary and a final answer """

from openai import OpenAI
client = OpenAI(api_key=os.environ["DEEPSEEK_API_KEY"],
                base_url="https://api.deepseek.com")

messages = [{"role": "system", "content": SYSTEM_PROMPT},
 {"role": "user", "content": "What is the meaning of life?"}]
response = client.chat.completions.create(
    model="deepseek-reasoner",
    messages=messages
)

reasoning_content = response.choices[0].message.reasoning_content #got back the reasong content
content = response.choices[0].message.content #adn content

Markdown(content)

Markdown(reasoning_content) #

"""## Pass it all into"""

#combinibg the whole thing into something we cna passinto an agent
CONTEXT = "<thinking>" + reasoning_content + "</thinking>" + "\n\n" + content

structured_results = await formatting_agent.run(CONTEXT) #back into our

type(structured_results.data.title)

Markdown(structured_results.data.title)

Markdown(structured_results.data.answer)

Markdown(structured_results.data.bullets)

Markdown(structured_results.data.thinking) #able to work out what was the thinkin tags etc

"""## Using a Reasoner Model as a Tool"""

#We gto an orchastion ganet, giving it tool,
## this prompt can be improved a lot. - currently its not returning everytime to the Reason Model for synthesis
ORCHESTRATOR_PROMPT = """You are an orchestration system that coordinates between specialized tools to produce comprehensive responses. Follow this exact sequence don't skip:

Once you have the search info always send back to the Reasoning Model for Synthesis

1. KEYWORD GENERATION
   Input: User query
   Tool: ReasoningEngine
   Action: Generate 3-5 search keywords/phrases
   Output format: List of {keyword, query_id}

2. SEARCH EXECUTION
   Input: Keywords from step 1
   Tool: SearchTool
   Action: Execute parallel searches using each keyword
   Output format: List of {query_id, search_results[]}

3. SYNTHESIS
   Input:
   - Original user query
   - All search results
   Tool: ReasoningEngine
   Action: Analyze and synthesize information if you need more info ask for more searches
   Output format: Structured report with:
   - Key findings
   - Supporting evidence
   - Confidence levels

4. RESPONSE FORMATTING
   Input: Synthesis report
   Action: Format into user-friendly response with:
   - Clear sections
   - Citations
   - Relevant metrics
   Output: Final formatted response

Required validation at each step:
- Verify tool outputs match expected formats
- Log any failed steps for retry
- Maintain traceability of information sources
"""

@dataclass
class TaskData:
    task: str

@dataclass
class SearchDataclass:
    max_results: int

class ReportStructuredResult(BaseModel):
    title: str = Field(description='This is a top level Markdown heading that covers the topic of the query and answer prefix it with #')
    answer: str = Field(description='This is a main section that provides answers for the query and questions')
    bullets: str = Field(description='This is a set of bulletpoints that summarize the answers for query')
    thinking: str = Field(description='This is a string that covers the thinking behind the answer')

orchestrator_agent = Agent('google-gla:gemini-1.5-flash',
              result_type=ReportStructuredResult,
              system_prompt=ORCHESTRATOR_PROMPT
              )

deps = SearchDataclass(max_results=3)

SYSTEM_PROMPT="""Your a helpful and wise reasoning assistant, you are an expert in thinking \
If you are given a question you think carefully and then respond with a title, \
a set of bullets points summary and a final answer """

@orchestrator_agent.tool_plain #
async def get_reasoning_answers(task: str) -> dict[str, Any]:
    """Get a strong reasoning answer for any task .

    Args:
        task: task for reasoning
    """
    client = OpenAI(api_key=os.environ["DEEPSEEK_API_KEY"],
                base_url="https://api.deepseek.com")


    messages = [{"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": task}]
    response = client.chat.completions.create(
        model="deepseek-reasoner",
        messages=messages
    )

    reasoning_content = response.choices[0].message.reasoning_content #calls deepseek reasoner, gets both the thiking etc, joins and send back to orhcestartor etc
    content = response.choices[0].message.content

    formatted_response = "<thinking>" + reasoning_content + "</thinking>" + "\n\n" + content

    return formatted_response



@orchestrator_agent.tool #Tavily
async def get_search(search_data:RunContext[SearchDataclass],query: str, query_number: int) -> dict[str, Any]:
    """Get the search for a keyword query.

    Args:
        query: keywords to search.
    """
    print(f"Search query {query_number}: {query}")
    max_results = search_data.deps.max_results
    results = await tavily_client.get_search_context(query=query, max_results=max_results)

    return results

structured_results = await orchestrator_agent.run("Please create me a report on GRPO RL used in the DeepSeekR1-Zero model", deps=deps)

structured_results

Markdown(structured_results.data.title)

Markdown(structured_results.data.bullets)

debug(structured_results)

